# Crawler <img align="" src="https://github.com/Maxsmile123/Maxsmile123/blob/333a0368f66c4b37dfefea27ff1833aba50d7ad3/res/hacker.png" height="25px" width="25px">

**Crawler** - по своей сути, это парсер сайтов для нахождения интересных папок сервера, либо скрытых ссылок.

`crawler.py` - ищет папки на сервере, подставляя популярные названия папок к домену </br>
`spider.py` - рекурсивно ищет все ссылки, находящиеся на странице, на случай, если разработчик оставил на сайте какую-то не отображающуюся ссылку.

## Использование
В результате вы получите вывод всего в консоль и файл с этими URL'ами. </br>
Используются следующие параметры:

**-o a.k.a. -logoff** - отключить вывод в консоль (результат будет только в файле) </br>

**-l a.k.a. -link** - обязательный параметр - ссылка на исходный сайт. В `crawler.py` указывается без http/https, в `spider.py` нужно указывать полный URL.

---

**Скрипт является кроссплатформенным — работает на Windows, Linux, MacOS**

